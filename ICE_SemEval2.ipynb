{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec, Vocab\n",
    "from gensim import utils, matutils\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from collections import defaultdict\n",
    "from copy import copy\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "infile = \"wikipedia_sentences_tokenised.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load_word2vec_format(\"/Users/tejus/Downloads/glove/glove.6B.300d.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "art\n",
      "authority\n",
      "bar\n",
      "blind\n",
      "bum\n",
      "chair\n",
      "channel\n",
      "child\n",
      "church\n",
      "circuit\n",
      "colourless\n",
      "cool\n",
      "day\n",
      "detention\n",
      "dyke\n",
      "facility\n",
      "faithful\n",
      "fatigue\n",
      "feeling\n",
      "fine\n",
      "fit\n",
      "free\n",
      "graceful\n",
      "green\n",
      "grip\n",
      "hearth\n",
      "holiday\n",
      "lady\n",
      "local\n",
      "material\n",
      "mouth\n",
      "nation\n",
      "natural\n",
      "nature\n",
      "oblique\n",
      "post\n",
      "restraint\n",
      "sense\n",
      "simple\n",
      "solemn\n",
      "spade\n",
      "stress\n",
      "vital\n",
      "yew\n",
      "begin\n",
      "call\n",
      "carry\n",
      "collaborate\n",
      "develop\n",
      "draw\n",
      "dress\n",
      "drift\n",
      "drive\n",
      "face\n",
      "ferret\n",
      "find\n",
      "keep\n",
      "leave\n",
      "live\n",
      "match\n",
      "play\n",
      "pull\n",
      "replace\n",
      "see\n",
      "serve\n",
      "strike\n",
      "train\n",
      "treat\n",
      "turn\n",
      "use\n",
      "wander\n",
      "wash\n",
      "work\n",
      "set(['blind', 'art', 'serve', 'facility', 'simple', 'oblique', 'bum', 'draw', 'authority', 'replace', 'wash', 'see', 'church', 'carry', 'graceful', 'chair', 'keep', 'fine', 'dyke', 'spade', 'use', 'develop', 'faithful', 'fit', 'solemn', 'dress', 'find', 'live', 'vital', 'call', 'yew', 'strike', 'holiday', 'local', 'channel', 'treat', 'grip', 'begin', 'collaborate', 'sense', 'nature', 'restraint', 'material', 'drift', 'detention', 'free', 'nation', 'stress', 'ferret', 'train', 'fatigue', 'colourless', 'child', 'natural', 'post', 'lady', 'day', 'cool', 'play', 'pull', 'hearth', 'bar', 'wander', 'work', 'drive', 'face', 'leave', 'turn', 'green', 'match', 'circuit', 'mouth', 'feeling'])\n"
     ]
    }
   ],
   "source": [
    "scws_ratings_file = \"wordlist.txt\"\n",
    "\n",
    "sims = defaultdict(list)\n",
    "vocab = set([])\n",
    "scores_list = []\n",
    "sim_list = []\n",
    "\n",
    "for line_num, line in enumerate(open(scws_ratings_file)):\n",
    "    tokens = line.split(\".\")\n",
    "    #print tokens\n",
    "    w1 = tokens[0]\n",
    "    print w1\n",
    "    #print w2\n",
    "    #sent1, sent2 = tokens[5: 7]\n",
    "    #score = float(tokens[7])\n",
    "\n",
    "    if w1 in model.vocab:\n",
    "    #    sims[w1].append((w2, score))\n",
    "     #   sims[w2].append((w1, score))\n",
    "        vocab.add(w1)\n",
    "        # vocab.add(w2)\n",
    "\n",
    "print vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idf_values = dict()\n",
    "for line in open(\"idf_values.txt\"):\n",
    "    word, value = line.strip().split(\"\\t\")\n",
    "    idf_values[word] = float(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "6000000\n",
      "7000000\n",
      "8000000\n",
      "9000000\n",
      "10000000\n",
      "11000000\n",
      "12000000\n",
      "13000000\n",
      "14000000\n",
      "15000000\n",
      "16000000\n",
      "17000000\n",
      "18000000\n",
      "19000000\n",
      "20000000\n",
      "21000000\n",
      "22000000\n",
      "23000000\n",
      "24000000\n",
      "25000000\n",
      "26000000\n",
      "27000000\n",
      "28000000\n",
      "29000000\n",
      "30000000\n",
      "31000000\n",
      "32000000\n",
      "33000000\n",
      "34000000\n",
      "35000000\n",
      "36000000\n",
      "37000000\n",
      "38000000\n",
      "39000000\n",
      "40000000\n",
      "41000000\n",
      "42000000\n",
      "43000000\n",
      "44000000\n",
      "45000000\n",
      "46000000\n",
      "47000000\n",
      "48000000\n",
      "49000000\n",
      "50000000\n",
      "51000000\n",
      "52000000\n",
      "53000000\n",
      "54000000\n",
      "55000000\n",
      "56000000\n",
      "57000000\n",
      "58000000\n",
      "59000000\n",
      "60000000\n",
      "61000000\n",
      "62000000\n",
      "63000000\n",
      "64000000\n",
      "65000000\n",
      "66000000\n",
      "67000000\n",
      "68000000\n",
      "69000000\n",
      "70000000\n",
      "71000000\n",
      "72000000\n",
      "73000000\n",
      "74000000\n",
      "75000000\n",
      "76000000\n",
      "77000000\n",
      "78000000\n",
      "79000000\n",
      "80000000\n",
      "81000000\n",
      "82000000\n",
      "83000000\n",
      "84000000\n",
      "85000000\n",
      "86000000\n",
      "87000000\n",
      "88000000\n",
      "89000000\n",
      "90000000\n",
      "91000000\n",
      "92000000\n",
      "93000000\n",
      "94000000\n",
      "95000000\n",
      "96000000\n",
      "97000000\n",
      "98000000\n",
      "99000000\n",
      "100000000\n",
      "101000000\n",
      "102000000\n",
      "103000000\n",
      "104000000\n",
      "105000000\n"
     ]
    }
   ],
   "source": [
    "context_dict = defaultdict(list)\n",
    "wiki_lines = []\n",
    "line_id = -1\n",
    "\n",
    "for line_num, line in enumerate(open(infile)):\n",
    "    line = line.lower()\n",
    "    if line_num % 1000000 == 0:\n",
    "        print line_num\n",
    "    \n",
    "    sent_words_set = set(line.strip().split())\n",
    "    new_set = sent_words_set.intersection(vocab)\n",
    "    \n",
    "    if new_set:\n",
    "        line_id += 1\n",
    "        wiki_lines.append(line)\n",
    "        for word in new_set:\n",
    "            context_dict[word].append(line_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2index = dict()\n",
    "for index, word in enumerate(model.index2word):\n",
    "    word2index[word] = index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_set = set()\n",
    "for line in open(\"stopwords.txt\"):\n",
    "    stop_set.add(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.syn0norm = normalize(model.syn0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#context_dict['holy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing(line):\n",
    "    line = line.strip()\n",
    "    line = line.replace(\".\", \"\")\n",
    "    line = line.replace(\",\", \"\")\n",
    "    line = line.replace('\"','')\n",
    "    \n",
    "    #regex = r\"(?u)\\b\\w\\w+\\b\"\n",
    "\n",
    "    words_list = []\n",
    "    for word in line.split():\n",
    "        if word not in stop_set:\n",
    "            words_list.append(word)\n",
    "\n",
    "    #return re.findall(regex, doc)\n",
    "    return words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window = 2\n",
    "num_clusters = 5\n",
    "topn = 5\n",
    "word_centroids = defaultdict(list)\n",
    "target_words_completed = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49, Calculating for word = colourless\n",
      "50, Calculating for word = child\n",
      "51, Calculating for word = bar\n",
      "52, Calculating for word = post\n",
      "53, Calculating for word = lady\n",
      "54, Calculating for word = day\n",
      "55, Calculating for word = cool\n",
      "56, Calculating for word = pull\n",
      "57, Calculating for word = stress\n",
      "58, Calculating for word = natural\n",
      "59, Calculating for word = work\n",
      "60, Calculating for word = drift\n",
      "61, Calculating for word = drive\n",
      "62, Calculating for word = draw\n",
      "63, Calculating for word = keep\n",
      "64, Calculating for word = leave\n",
      "65, Calculating for word = turn\n",
      "66, Calculating for word = green\n",
      "67, Calculating for word = match\n",
      "68, Calculating for word = circuit\n",
      "69, Calculating for word = bum\n",
      "70, Calculating for word = dyke\n",
      "71, Calculating for word = fatigue\n",
      "72, Calculating for word = feeling\n"
     ]
    }
   ],
   "source": [
    "for j_index, t_word in enumerate(context_dict):\n",
    "    if t_word in target_words_completed:\n",
    "        continue\n",
    "    \n",
    "    print \"{}, Calculating for word = {}\".format(j_index, t_word)\n",
    "    context_vectors_list = list()\n",
    "    cluster_centroids_list = []\n",
    "    \n",
    "    for line_id in context_dict[t_word]:\n",
    "\n",
    "        line = wiki_lines[line_id]\n",
    "        words_list = preprocessing(line)    \n",
    "\n",
    "        if( len(words_list) < 5 ):\n",
    "            continue\n",
    "\n",
    "        for i, word in enumerate(words_list):\n",
    "            if word == t_word:\n",
    "                sum_vector = np.zeros(300)\n",
    "                total_context_words = 0\n",
    "\n",
    "                left_window = words_list[max(i - window, 0): i]\n",
    "                left_index = list(range(1, len(left_window) + 1))\n",
    "                left_index.reverse()\n",
    "                left_window_2 = zip(left_index, left_window)\n",
    "\n",
    "                right_window = words_list[i + 1: i + (window + 1)]\n",
    "                right_index = list(range(1, len(right_window) + 1))\n",
    "                right_window_2 = zip(right_index, right_window)\n",
    "\n",
    "                aggregate_list = left_window_2 + right_window_2\n",
    "\n",
    "                for weight, context_word in aggregate_list:\n",
    "                    if context_word in word2index:\n",
    "                        total_context_words += 1\n",
    "                        context_word_vector = model.syn0norm[word2index[context_word]]\n",
    "                        current_word_vector = model.syn0norm[word2index[word]]\n",
    "                        similarity = np.dot(context_word_vector, current_word_vector)\n",
    "\n",
    "                        if context_word in idf_values:\n",
    "                            sum_vector += idf_values[context_word] * similarity * context_word_vector\n",
    "\n",
    "                if total_context_words > 0:\n",
    "                    #avg_vector = (1. / total_context_words) * (sum_vector)\n",
    "                    avg_vector = sum_vector\n",
    "                    context_vectors_list.append(avg_vector)\n",
    "\n",
    "    if len(context_vectors_list) == 0:\n",
    "        target_words_completed.add(t_word)\n",
    "        continue\n",
    "    \n",
    "    context_vectors_array = normalize(np.array(context_vectors_list))\n",
    "    \n",
    "    if len(context_vectors_array > 5000):\n",
    "        context_vectors_subset = context_vectors_array[np.random.choice(context_vectors_array.shape[0], size=500, replace=False), :]\n",
    "    else:\n",
    "        context_vectors_subset = context_vectors_array\n",
    "        \n",
    "    kmeans = KMeans(n_clusters=num_clusters, n_jobs=1, n_init=2).fit(context_vectors_subset)\n",
    "\n",
    "    for j in range(num_clusters):\n",
    "        flag = False\n",
    "        dists = np.dot(model.syn0, kmeans.cluster_centers_[j])\n",
    "        best = matutils.argsort(dists, topn=topn, reverse=True)\n",
    "\n",
    "        # ignore (don't return) words from the input\n",
    "        result = []\n",
    "        for sim in best:\n",
    "            word = model.index2word[sim]\n",
    "            if word in stop_set:\n",
    "                flag = True\n",
    "                break\n",
    "            result.append( word + \" \" + str(float(dists[sim])))\n",
    "\n",
    "        if flag:\n",
    "            continue\n",
    "        \n",
    "        cluster_centroids_list.append(kmeans.cluster_centers_[j])\n",
    "        #print \"\\n\\n\", \"Cluster {}\".format(j)\n",
    "        #print \"\\n\".join(result[:topn])\n",
    "    word_centroids[t_word] = copy(cluster_centroids_list)\n",
    "    joblib.dump(np.array(cluster_centroids_list), \"wikipedia_context_centroids_senseval/{}.pkl\".format(t_word) )\n",
    "    target_words_completed.add(t_word)\n",
    "#     clf = joblib.load(\"wikipedia_context_centroids_senseval/{}.pkl\".format(t_word))\n",
    "#     print clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'art',\n",
       " 'authority',\n",
       " 'bar',\n",
       " 'begin',\n",
       " 'blind',\n",
       " 'bum',\n",
       " 'call',\n",
       " 'carry',\n",
       " 'chair',\n",
       " 'channel',\n",
       " 'child',\n",
       " 'church',\n",
       " 'circuit',\n",
       " 'collaborate',\n",
       " 'colourless',\n",
       " 'cool',\n",
       " 'day',\n",
       " 'detention',\n",
       " 'develop',\n",
       " 'draw',\n",
       " 'dress',\n",
       " 'drift',\n",
       " 'drive',\n",
       " 'dyke',\n",
       " 'face',\n",
       " 'facility',\n",
       " 'faithful',\n",
       " 'fatigue',\n",
       " 'feeling',\n",
       " 'ferret',\n",
       " 'find',\n",
       " 'fine',\n",
       " 'fit',\n",
       " 'free',\n",
       " 'graceful',\n",
       " 'green',\n",
       " 'grip',\n",
       " 'hearth',\n",
       " 'holiday',\n",
       " 'keep',\n",
       " 'lady',\n",
       " 'leave',\n",
       " 'live',\n",
       " 'local',\n",
       " 'match',\n",
       " 'material',\n",
       " 'mouth',\n",
       " 'nation',\n",
       " 'natural',\n",
       " 'nature',\n",
       " 'oblique',\n",
       " 'play',\n",
       " 'post',\n",
       " 'pull',\n",
       " 'replace',\n",
       " 'restraint',\n",
       " 'see',\n",
       " 'sense',\n",
       " 'serve',\n",
       " 'simple',\n",
       " 'solemn',\n",
       " 'spade',\n",
       " 'stress',\n",
       " 'strike',\n",
       " 'train',\n",
       " 'treat',\n",
       " 'turn',\n",
       " 'use',\n",
       " 'vital',\n",
       " 'wander',\n",
       " 'wash',\n",
       " 'work',\n",
       " 'yew'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_words_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
