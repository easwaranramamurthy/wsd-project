\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

\usepackage[final]{nips_2016}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2016}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}   
\usepackage{amsmath}   % microtypography
%\usepackage[final]{nips_2016}
\title{Word Sense Induction and Disambiguation using Context Embeddings}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.
%\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
%\norm{ \biggl(\sum_{n=1}^N \mathbf{P}_{n}\biggr) }

\author{
  Easwaran Ramamurthy \\
  %\thanks{Use footnote for providing further
%    information about author (webpage, alternative
%    address)---\emph{not} for acknowledging funding agencies.} \\
%  Department of Computer Science\\
%  Cranberry-Lemon University\\
%  Pittsburgh, PA 15213 
  \texttt{eramamur@andrew.cmu.edu} \\
  %% examples of more authors
   \And
  Devendra Singh Sachan \\
  %% Affiliation \\
  %% Address \\
  \texttt{dsachan@andrew.cmu.edu} \\
  \AND
  Tejus Siddagangaiah \\
  %% Affiliation \\
  %% Address \\
  \texttt{tsiddaga@andrew.cmu.edu} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

%\renewcommand{\bibsection}{\chapter{\bibname}}
%\newcommand{\bibpreamble}{This text goes between the ``Bibliography''
%  header and the actual list of references}
%\bibstyle{unsrt}
\nocite{*}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle
\bibliographystyle{unsrt}
\begin{abstract}
Recent years have seen the advent of word embedding models as a popular choice for distributional representation of word meaning. These embedding representations are increasingly being used by the community to improve on NLP tasks such as word sense induction (WSI) and word sense disambiguation (WSD). Existing approaches to solve these problems use hand crafted features to represent and cluster context words. We propose to evaluate current methods and develop a new approach that attempts to learn feature representation of context words using deep learning methods such as convolutional neural networks (CNN) and Long short term memory (LSTM).
\end{abstract}

\section{Introduction}

Word sense disambiguation (WSD) and word sense induction (WSI) aim at identifying the correct sense of a polysemous or homonymous word when used in a sentence. Polysemous words are those words which have different but related senses when used in different contexts. In contrast, homonymous words are those words which have totally different senses when used in different contexts. An example of a homonymous word is “bark” which has two completely different senses when used in the statements “My dog would always bark at mailmen” and “The tree’s bark was a rusty brown”. An example of polysemous word is the occurrence of the word “newspaper” in the two sentences “The newspaper got wet in the rain” and “The newspaper fired some of its editing staff”. Disambiguation of word senses is an important task that has several applications including machine translation, information retrieval and question answering.\\
Distributed representations of words using vector space models (VSMs) of lexical semantics have recently gained huge popularity in the field of natural language processing and natural language understanding. VSMs represent words as dense, real-valued vector embeddings in a space where semantically and syntactically similar words are closer to each other. Such representations have been shown to improve generalization on a variety of natural language processing tasks (Bengio et al. 2003; Mnih and Hinton, 2007; Collobert and Weston, 2008) specially when there is an opportunity to train on very large corpuses. The recent considerable interest in the CBOW and Skip-gram models of Mikolov et al (2013a), collectively know as word2vec stems from the fact that they are simple log-linear models that can produce word embeddings of high quality from the entire Wikipedia English corpus in less than half a day on a single machine. One notable deficiency in prior work is that these cannot model multiple senses of each word thereby ignoring polysemes and homonyms. In this report, we discuss some of the background work in this area focusing on solving the multi-sense embedding problem, followed by a discussion of some promising methods that can be improved upon. We then propose to come up with ways to learn context embeddings automatically from word embeddings which can be an improvement over traditional approaches that use \textit{tf-idf} or $chi^2$ weighted features. 

\section{Related Work}
Firth’s distributional hypothesis states that the semantics of a word is reflected in the contexts in which it occurs. Discovering multi-sense word embeddings can therefore be useful for tasks such as WSI and WSD. This is exactly the focus of the seminal paper by Reisinger and Mooney (2010) where they use efficient unsupervised learning to cluster context-specific embeddings of words to produce multiple “sense-specific” prototypes for each word which can capture homonymy as well as polysemy. These multi-prototype models are defined by clustering feature vectors $v(c)$ for each context $c in C(w)$ that appears in the corpus. These feature vectors can either be \textit{tf-idf} or $\chi^2$ weighted features of words appearing in a window of length $S$ around the given word. Semantic similarity between two isolated words or also two words in context can then be measured using different distance measures like average similarity between all pairs of clusters for two words or maximum similarity calculated between the most likely clusters identified for a word in a given context. However, this approach provided no method to select potentially many vectors that capture context specificity for downstream NLP classification tasks.\\
Then followed the work of Huang et al. which generates context embeddings from pre-trained neural word embeddings followed by computing \textit{tf-idf} weighted sum of words in a context followed by clustering of each context embedding. Each occurrence of the word is then re-labeled using the learned clusters followed by training of a new word model on the labeled embeddings generating sense embeddings. Trask et al’s sense2vec program expanded on this work by eliminating the need for training embedding multiple times and also eliminating the need for a clustering step. The sense2vec model predicts a word sense given surrounding word senses and is very useful in creating efficient word-sense embeddings that can be ingested by a supervised classifier.


\section{Methods}
Existing methods use a linear combination of tf-idf weighted word embeddings for creating vectors for context words and apply some sort of clustering algorithm such as K-Means to identify the number of senses of a word. Instead, learning feature representations for context words using deep architectures has been shown to be more efficient in NLU tasks. We plan to replicate the work of Yuan et al \cite{yuan2016word} for WSD and use it as a baseline for our experiments. We also propose to learn unsupervised feature representations of contexts by training convolutional neural network (CNN) and LSTMs on word embeddings. One-hot encoding of the target words could be used to make predictions. Evaluation of our learned representation could be performed on the below mentioned datasets and compared with the earlier state of the art works on these topics.


\section{Dataset Description and Evaluation}
%\begin{itemize}
\textbf{SCWS\footnote{http://ai.stanford.edu/~ehhuang/}:} Stanford’s Contextual Word Similarities (SCWS) is a set of 2003 word pairs and their sentential contexts. Each instance in the dataset consists of a pair of words and respective POS tags. The dataset also consists of similarity ratings between the words that are collected from averaging human ratings and also ten specific individual ratings. One possible way to evaluate our model on this dataset is to compute Spearman’s rank correlation coefficient between the assigned human ratings and the cosine similarity of the two computed word sense vectors.\\

\textbf{SemEval 2013, Task-13\footnote{https://www.cs.york.ac.uk/semeval-2013/task13.html}:} This dataset is drawn from the Open American National Corpus (OANC) which includes text of all genres. It has 50 target lemmas which consists of 20 nouns, 20 verbs and 10 adjectives and has 4664 total instances. It seeks to determine the senses of a word in a fully unsupervised manner. In this, the task is to annotate each instance of a target word with one or more of their senses using either WordNet 3.1 sense inventory or an induced sense inventory. Evaluation can be done using three metrics, namely the Jaccard similarity which can used to measure agreement between senses, positionally-weighted Kendall’s Tau similarity to rank senses by their applicability and weighted NDCG to measure agreement with human annotators.\\

%\textbf{SemEval 2015, Task-13\footnote{http://alt.qcri.org/semeval2015/task13/}:} This dataset consists of tokenized, POS tagged documents from Babelnet in three languages. It contains both named entities and word sense inventories. The task is to annotate all the words with their corresponding senses. To evaluate the performance, we can use precision, recall and F1 metrics.\\

\textbf{MSH\footnote{https://wsd.nlm.nih.gov/collaboration.shtml}:} The MSH WSD dataset consists of lexical information for medical research publications in PubMed. It is an automatically generated dataset specifically aimed at being a resource for testing WSD algorithms that capture the complexity associated with ambiguous medical terms. It consists of a total of 203 ambiguous words which include 106 ambiguous abbreviations and 88 ambiguous terms and 9 of which are a combination of both. For each ambiguous term or abbreviation, it contains a maximum of 100 labeled instances that include the title and abstract of the publication in MEDLINE and the word sense associated with the ambiguous word. Since the dataset is annotated with labels, simple metrics like accuracy, precision, recall, F1 and auROC can be used to evaluate WSD models on this dataset.\\
%\end{itemize}

\section{Work division and Timeline}
Firstly, we aim to understand and implement the previous work in this area and obtain a baseline on the above datasets. This will be done for the approaches outlined in Yuan et al\cite{yuan2016word}, Neelkantan et al\cite{neelakantan2015efficient} and Trask et al\cite{trask2015sense2vec} and each team member will take up responsibility for one approach. The aim is to accomplish this work by November 7th, 2016 following which we will work on the above mentioned plan to come up with new approaches, and compare the results with the baselines.




\begin{equation}\label{eq:perceptron}
\begin{split}
& maximize_{{\alpha_i^{(j)}}, {\phi_i}} \sum_{j=1}^m \norm{ {\mathbf{x}^{(j)}} - {\sum_{i=1}^k {{\alpha_i^{(j)}}{\phi_i}}} }^2 + \lambda{\sum_{i=1}^k S(\alpha_i^{(j)})}  \\
& \quad\quad{subject\quad{to}} \qquad {\lVert \phi_i \rVert}^2 \leq 1,\quad for\quad i = 1,2,...k
\end{split}
\end{equation}

\pagebreak
%\renewcommand{\bibsection}{\chapter{\bibname}}
%\newcommand{\bibpreamble}{This text goes between the ``Bibliography''
%  header and the actual list of references}
%\bibstyle{unsrt}
\nocite{*}
\bibliography{reference}


\end{document}
