\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

\usepackage[final]{nips_2016}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2016}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
%\usepackage[final]{nips_2016}
\title{Word Sense Induction and Disambiguation using Context Embeddings}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.


\author{
  Easwaran Ramamurthy \\
  %\thanks{Use footnote for providing further
%    information about author (webpage, alternative
%    address)---\emph{not} for acknowledging funding agencies.} \\
%  Department of Computer Science\\
%  Cranberry-Lemon University\\
%  Pittsburgh, PA 15213 
  \texttt{eramamur@andrew.cmu.edu} \\
  %% examples of more authors
   \And
  Devendra Singh Sachan \\
  %% Affiliation \\
  %% Address \\
  \texttt{dsachan@andrew.cmu.edu} \\
  \AND
  Tejus Siddagangaiah \\
  %% Affiliation \\
  %% Address \\
  \texttt{tsiddaga@andrew.cmu.edu} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

%\renewcommand{\bibsection}{\chapter{\bibname}}
%\newcommand{\bibpreamble}{This text goes between the ``Bibliography''
%  header and the actual list of references}
%\bibstyle{unsrt}
\nocite{*}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle
\bibliographystyle{unsrt}
\begin{abstract}
Recent years have seen the advent of word embedding models as a popular choice for distributional representation of word meaning. These embedding representations are increasingly being used by the community to improve on NLP tasks such as word sense induction (WSI) and word sense disambiguation (WSD). Existing approaches to solve these problems use hand crafted features to represent and cluster context words. We propose to evaluate current methods and develop a new approach that attempts to learn feature representation of context words using deep learning methods such as convolutional neural networks (CNN) and Long short term memory (LSTM).
\end{abstract}

\section{Problem Definition}

Word sense induction (WSI) and word sense disambiguation (WSD) are long standing problems in natural language understanding (NLU) and aim at identifying the correct sense of a polysemic word when used in a sentence. A polysemic word is one which can have multiple meanings when used in different contexts. For example, consider the usage of the word “cold” in the phrases “He has a cold” and “The ice is cold”. Specifically, in the first phrase, the usage of the word “cold” points to the common cold and in the second case it points to cold temperature. In WSI, the task is to automatically infer the number of senses of a word while in WSD, the task is to output the correct sense for that word given a fixed set of word senses with their usage. Resolving such ambiguities through WSI and WSD has several applications in fields like information retrieval, machine translation and question answering.

\section{Related Work}

Recently, word embedding models \cite{mikolov2013efficient} have been gaining popularity in natural language processing tasks like parsing, machine translation and text classification. Such models aim to learn a representation of every word as a vector in a latent space where words that share similar contexts in the corpus are closer to each other. More recently, Yuan et al \cite{yuan2016word} reported state of the art results on a WSD task on the New Oxford American Dictionary (NOAD) using a word embedding representation as input to an LSTM neural network language model followed by a label propagation method on the resulting graph. Trask et al \cite{trask2015sense2vec} learned embeddings of words and their POS tags to do WSD in which the number of senses of a word is determined by its different POS tags. Neelkantan et al \cite{neelakantan2015efficient} proposed a Non-Parametric Multi Sense Skip-gram model in which they learn multiple embeddings of polysemous words using context clustering. The approach of Reisinger et al\cite{reisinger2010multi} provides a context dependent vector representation of word meaning using clustering for homonymy and polysemy words. We now describe what we propose to achieve through this project building up on the work done in the papers cited above.

\section{Methods}
Existing methods use a linear combination of tf-idf weighted word embeddings for creating vectors for context words and apply some sort of clustering algorithm such as K-Means to identify the number of senses of a word. Instead, learning feature representations for context words using deep architectures has been shown to be more efficient in NLU tasks. We plan to replicate the work of Yuan et al \cite{yuan2016word} for WSD and use it as a baseline for our experiments. We also propose to learn unsupervised feature representations of contexts by training convolutional neural network (CNN) and LSTMs on word embeddings. One-hot encoding of the target words could be used to make predictions. Evaluation of our learned representation could be performed on the below mentioned datasets and compared with the earlier state of the art works on these topics.


\section{Dataset Description and Evaluation}
%\begin{itemize}
\textbf{SCWS\footnote{http://ai.stanford.edu/~ehhuang/}:} Stanford’s Contextual Word Similarities (SCWS) is a set of 2003 word pairs and their sentential contexts. Each instance in the dataset consists of a pair of words and respective POS tags. The dataset also consists of similarity ratings between the words that are collected from averaging human ratings and also ten specific individual ratings. One possible way to evaluate our model on this dataset is to compute Spearman’s rank correlation coefficient between the assigned human ratings and the cosine similarity of the two computed word sense vectors.\\

\textbf{SemEval 2013, Task-13\footnote{https://www.cs.york.ac.uk/semeval-2013/task13.html}:} This dataset is drawn from the Open American National Corpus (OANC) which includes text of all genres. It has 50 target lemmas which consists of 20 nouns, 20 verbs and 10 adjectives and has 4664 total instances. It seeks to determine the senses of a word in a fully unsupervised manner. In this, the task is to annotate each instance of a target word with one or more of their senses using either WordNet 3.1 sense inventory or an induced sense inventory. Evaluation can be done using three metrics, namely the Jaccard similarity which can used to measure agreement between senses, positionally-weighted Kendall’s Tau similarity to rank senses by their applicability and weighted NDCG to measure agreement with human annotators.\\

%\textbf{SemEval 2015, Task-13\footnote{http://alt.qcri.org/semeval2015/task13/}:} This dataset consists of tokenized, POS tagged documents from Babelnet in three languages. It contains both named entities and word sense inventories. The task is to annotate all the words with their corresponding senses. To evaluate the performance, we can use precision, recall and F1 metrics.\\

\textbf{MSH\footnote{https://wsd.nlm.nih.gov/collaboration.shtml}:} The MSH WSD dataset consists of lexical information for medical research publications in PubMed. It is an automatically generated dataset specifically aimed at being a resource for testing WSD algorithms that capture the complexity associated with ambiguous medical terms. It consists of a total of 203 ambiguous words which include 106 ambiguous abbreviations and 88 ambiguous terms and 9 of which are a combination of both. For each ambiguous term or abbreviation, it contains a maximum of 100 labeled instances that include the title and abstract of the publication in MEDLINE and the word sense associated with the ambiguous word. Since the dataset is annotated with labels, simple metrics like accuracy, precision, recall, F1 and auROC can be used to evaluate WSD models on this dataset.\\
%\end{itemize}

\section{Work division and Timeline}
Firstly, we aim to understand and implement the previous work in this area and obtain a baseline on the above datasets. This will be done for the approaches outlined in Yuan et al\cite{yuan2016word}, Neelkantan et al\cite{neelakantan2015efficient} and Trask et al\cite{trask2015sense2vec} and each team member will take up responsibility for one approach. The aim is to accomplish this work by November 7th, 2016 following which we will work on the above mentioned plan to come up with new approaches, and compare the results with the baselines.



%\renewcommand{\bibsection}{\chapter{\bibname}}
%\newcommand{\bibpreamble}{This text goes between the ``Bibliography''
%  header and the actual list of references}
%\bibstyle{unsrt}
\nocite{*}
\bibliography{reference}


\end{document}
