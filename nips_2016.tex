\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

\usepackage[final]{nips_2016}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2016}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
%\usepackage[final]{nips_2016}
\title{Word Sense Induction and Disambiguation using Context Embeddings}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.


\author{
  Easwaran Ramamurthy \\
  %\thanks{Use footnote for providing further
%    information about author (webpage, alternative
%    address)---\emph{not} for acknowledging funding agencies.} \\
%  Department of Computer Science\\
%  Cranberry-Lemon University\\
%  Pittsburgh, PA 15213 
  \texttt{eramamur@andrew.cmu.edu} \\
  %% examples of more authors
   \And
  Devendra Singh Sachan \\
  %% Affiliation \\
  %% Address \\
  \texttt{dsachan@andrew.cmu.edu} \\
  \AND
  Tejus Siddagangaiah \\
  %% Affiliation \\
  %% Address \\
  \texttt{tsiddaga@andrew.cmu.edu} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\renewcommand{\bibsection}{\chapter{\bibname}}
%\newcommand{\bibpreamble}{This text goes between the ``Bibliography''
%  header and the actual list of references}
%\bibstyle{unsrt}
\nocite{*}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle
\bibliographystyle{unsrt}
\begin{abstract}
  The abstract paragraph should be indented \nicefrac{1}{2}~inch
  (3~picas) on both the left- and right-hand margins. Use 10~point
  type, with a vertical spacing (leading) of 11~points.  The word
  \textbf{Abstract} must be centered, bold, and in point size 12. Two
  line spaces precede the abstract. The abstract must be limited to
  one paragraph.
\end{abstract}

\section{Problem Definition}

Word sense induction (WSI) and word sense disambiguation (WSD) are long standing problems in natural language understanding (NLU) and aim at identifying the correct sense of a polysemic word when used in a sentence. A polysemic word is one which can have multiple meanings when used in different contexts. For example, consider the usage of the word “cold” in the phrases “He has a cold” and “The ice is cold”. Specifically, in the first phrase, the usage of the word “cold” points to the common cold and in the second case it points to cold temperature. In WSI, the task is to automatically infer the number of senses of a word while in WSD, the task is to output the correct sense for that word given a fixed set of word senses with their usage. Resolving such disambiguations through WSI and WSD has several applications in fields like information retrieval, machine translation and question answering.

\section{Related Work}

Recently, word embedding models (Mikolov et al., 2013b) have been gaining popularity in natural language processing tasks like parsing, machine translation and text classification. Such models aim to learn a representation of every word as a vector in a latent space where words that share similar contexts in the corpus are closer to each other. More recently, Yuan et al (Yuan et al 2015) reported state of the art results on a WSD task on the New Oxford American Dictionary (NOAD) using a word embedding representation as input to an LSTM neural network language model followed by a label propagation method on the resulting graph. Trask et al (ICLR 2016) learned embeddings of words and their POS tags to do WSD in which the number of senses of a word is determined by its different POS tags. Neelkantan et al () proposed a Non-Parametric Multi Sense Skip-gram model in which they learn multiple embeddings of polysemous words using context clustering. The approach of Reisinger et al  provides a context dependent vector representation of word meaning using clustering for homonymy and polysemy words. We now describe what we propose to achieve through this project building up on the work done in the papers cited above.

\section{Method}
Most of these earlier works use a linear combination of tf-idf weighted word embeddings for creating context vectors for words and apply some sort of clustering algorithm such as K-Means to identify the number of senses of a word. We plan to replicate the work of Yuan et al on for WSD tasks and use it as a baseline for our experiments. We will also learn unsupervised feature representation of context by training convolutional neural network (CNN) and LSTM on word embeddings. The prediction will be done using one-hot encoding of the target word. We will evaluate our learned representation on the below mentioned datasets and compare it with the earlier state of the art works on these topics. 

\section{Dataset Description and Evaluation}
\begin{itemize}
\item \textbf{SCWS:} Stanford’s Contextual Word Similarities (SCWS) is a set of 2003 word pairs and their sentential contexts. Each instance in the dataset consists of a pair of words and respective POS tags. The dataset also consists of similarity ratings between the words that are collected from averaging human ratings and also ten specific individual ratings. One possible way to evaluate our model on this dataset is to compute Spearman’s rank correlation coefficient between the assigned human ratings and the cosine similarity of the two computed word sense vectors.

\item \textbf{SemEval 2013, Task-13:} This dataset is drawn from the Open American National Corpus (OANC) which includes text of all genres. It has 50 target lemmas which consists of 20 nouns, 20 verbs and 10 adjectives and has 4664 total instances. It seeks to determine the senses of a word in a fully unsupervised manner. In this, the task is to annotate each instance of a target word with one or more of their senses using either WordNet 3.1 sense inventory or an induced sense inventory. Evaluation can be done using three metrics, namely the Jaccard similarity which can used to measure agreement between senses, positionally-weighted Kendall’s Tau similarity to rank senses by their applicability and weighted NDCG to measure agreement with human annotators.

\item \textbf{SemEval 2015, Task-13:} This dataset consists of tokenized, POS tagged documents from Babelnet in three languages. It contains both named entities and word sense inventories. The task is to annotate all the words with their corresponding senses. To evaluate the performance, we can use precision, recall and F1 metrics.

\item \textbf{MSH:} The MSH WSD dataset consists of lexical information for medical research publications in PubMed. It is an automatically generated dataset specifically aimed at being a resource for testing WSD algorithms that capture the complexity associated with ambiguous medical terms. It consists of a total of 203 ambiguous words which include 106 ambiguous abbreviations and 88 ambiguous terms and 9 of which are a combination of both. For each ambiguous term or abbreviation, it contains a maximum of 100 labeled instances that include the title and abstract of the publication in MEDLINE and the word sense associated with the ambiguous word. Since the dataset is annotated with labels, simple metrics like accuracy, precision, recall, F1 and auROC can be used to evaluate WSD models on this dataset.
\end{itemize}


\section*{References}


%\renewcommand{\bibsection}{\chapter{\bibname}}
%\newcommand{\bibpreamble}{This text goes between the ``Bibliography''
%  header and the actual list of references}
%\bibstyle{unsrt}
\nocite{*}
\bibliography{reference}


\end{document}
